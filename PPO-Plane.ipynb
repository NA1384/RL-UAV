{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from src.algorithms.PPO import PPO\n",
    "from src.environments.jsbsim.JSBSimEnv import Env #can be jsbsim.JSBSimEnv or xplane.XPlaneEnv\n",
    "from src.scenarios.deltaAttitudeControlScene import Scene"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# Setting the matplotlib style with the seaborn module\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "# Hyperparameters of the PPO algorithm\n",
    "steps_per_epoch = 5000\n",
    "epochs = 2000\n",
    "gamma = 0.99\n",
    "clip_ratio = 0.2\n",
    "policy_learning_rate = 3e-4\n",
    "value_function_learning_rate = 1e-3\n",
    "train_policy_iterations = 80\n",
    "train_value_iterations = 80\n",
    "lam = 0.97\n",
    "target_kl = 0.01\n",
    "hidden_sizes = (128, 128)\n",
    "observation_dimensions = 7\n",
    "num_actions = 4\n",
    "\n",
    "savePeriod = 25  # every so many epochs the table/model will be saved to a file\n",
    "\n",
    "# Parameters for logging and actions\n",
    "pauseDelay = 0.1  # time an action is being applied to the environment\n",
    "id = \"doubleDeep\"\n",
    "\n",
    "experimentName = \"Experiment\"\n",
    "connectAttempts = 0.0  # counts everytime the UDP packages are lost on a single retry\n",
    "\n",
    "# add notes that will be saved to the setup file to clarify the experiment setup better\n",
    "notes = \"This experiment was run with...\"\n",
    "\n",
    "dateTime = str(time.ctime(time.time()))\n",
    "dateTime = dateTime.replace(\":\", \"-\")\n",
    "dateTime = dateTime.replace(\" \", \"_\")\n",
    "experimentName = experimentName + \"-\" + dateTime\n",
    "\n",
    "errors = 0.0  # counts everytime the UDP packages are lost on all retries\n",
    "\n",
    "timeStart = time.time()  # used to measure time\n",
    "timeEnd = time.time()  # used to measure time\n",
    "\n",
    "# Parameters for training and visualisation\n",
    "loadModel = False  # will load trained model for tf if True\n",
    "jsbRender = False # will send UDP data to flight gear for rendering if True\n",
    "jsbRealTime = False  # will slow down the physics to portrait real time rendering\n",
    "saveResultsToPlot = True  # Saves results to png in the experiment folder at runtime\n",
    "usePredefinedSeeds = False  # Sets seeds for tf, np and random for more replicable results\n",
    "# (not fully replicable due to stochastic environments)\n",
    "\n",
    "# Parameters for the environment and scenario\n",
    "startingVelocity = 60\n",
    "startingPitchRange = 10\n",
    "startingRollRange = 15\n",
    "randomDesiredState = True  # Set a new state to stabalize towards every episode\n",
    "desiredPitchRange = 1\n",
    "desiredRollRange = 1\n",
    "\n",
    "dictObservation = {\n",
    "    \"lat\": 0,\n",
    "    \"long\": 1,\n",
    "    \"alt\": 2,\n",
    "    \"pitch\": 3,\n",
    "    \"roll\": 4,\n",
    "    \"yaw\": 5,\n",
    "    \"gear\": 6}\n",
    "dictAction = {\n",
    "    \"pi+\": 0,\n",
    "    \"pi-\": 1,\n",
    "    \"ro+\": 2,\n",
    "    \"ro-\": 3,\n",
    "    \"ru+\": 4,\n",
    "    \"ru-\": 5,\n",
    "    \"no\": 6}\n",
    "dictErrors = {\n",
    "    \"reset\": 0,\n",
    "    \"update\": 0,\n",
    "    \"step\": 0}\n",
    "dictRotation = {\n",
    "    \"roll\": 0,\n",
    "    \"pitch\": 1,\n",
    "    \"yaw\": 2,\n",
    "    \"northVelo\": 3,\n",
    "    \"eastVelo\": 4,\n",
    "    \"verticalVelo\": 5}\n",
    "\n",
    "movingEpRewards = {\n",
    "    \"epoch\": [],\n",
    "    \"mreturn\": [],\n",
    "    \"mlength\": [],\n",
    "    \"average\": [],\n",
    "    \"reward\": [],\n",
    "    \"return\": [],\n",
    "    \"length\": []}\n",
    "\n",
    "epochRewards = []\n",
    "movingRate = savePeriod  # gives the number by which the moving average will be done, best if n * savePeriod\n",
    "\n",
    "fallbackState = [0] * observation_dimensions  # Used in case of connection error to XPlane\n",
    "fallbackState = [tuple(fallbackState)]\n",
    "\n",
    "# -998->NO CHANGE\n",
    "flightOrigin = [35.126, 126.809, 6000, 0, 0, 0, 1]  # Gwangju SK\n",
    "flightDestinaion = [33.508, 126.487, 6000, -998, -998, -998, 1]  # Jeju SK\n",
    "#  Other locations to use: Memmingen: [47.988, 10.240], Chicago: [41.976, -87.902]\n",
    "\n",
    "stateDepth = 1  # Number of old observations kept for current state. State will consist of s(t) ... s(t_n)\n",
    "\n",
    "\n",
    "if not os.path.exists(\"./Experiments/\" + experimentName):\n",
    "    os.makedirs(\"./Experiments/\" + experimentName)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Initialize scene, environment and PPO agent\n",
    "scene = Scene(dictObservation, dictAction, num_actions, stateDepth, startingVelocity, startingPitchRange,\n",
    "              startingRollRange, usePredefinedSeeds, randomDesiredState, desiredPitchRange, desiredRollRange)\n",
    "\n",
    "env = Env(scene, flightOrigin, flightDestinaion, num_actions, usePredefinedSeeds,\n",
    "          dictObservation, dictAction, dictRotation, startingVelocity, pauseDelay, id, jsbRender, jsbRealTime)\n",
    "\n",
    "P = PPO(steps_per_epoch, epochs, gamma, clip_ratio, policy_learning_rate, value_function_learning_rate,\n",
    "        train_policy_iterations, train_value_iterations, lam, target_kl, hidden_sizes, observation_dimensions,\n",
    "        num_actions, env, experimentName, loadModel)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Iterate over the number of epochs\n",
    "for epoch in range(epochs + 1):\n",
    "    # Initialize the sum of the returns, lengths and number of episodes for each epoch\n",
    "    sum_return = 0\n",
    "    sum_length = 0\n",
    "    num_episodes = 0\n",
    "    epochReward = 0\n",
    "\n",
    "    # Iterate over the steps of each epoch\n",
    "    for t in range(steps_per_epoch):\n",
    "\n",
    "        # Get the logits, action, and take one step in the environment\n",
    "        observation = P.observation.reshape(1, -1)\n",
    "        logits, action = P.sample_action(observation)\n",
    "        observation_new, reward, done, _ = env.step(action[0].numpy())\n",
    "        P.episode_return += reward\n",
    "        P.episode_length += 1\n",
    "        epochReward += reward\n",
    "\n",
    "        # checking if state includes a NaN (happens in JSBSim sometimes)\n",
    "        if np.isnan(observation_new).any():\n",
    "            if id == \"doubleDeep\":\n",
    "                newState = fallbackState\n",
    "            else:\n",
    "                newState = 0\n",
    "            reward = 0\n",
    "            info = [[0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], 0]\n",
    "            dictErrors[\"step\"] = \"NaN in state\"\n",
    "            errors += 1\n",
    "            done = True\n",
    "\n",
    "        # Get the value and log-probability of the action\n",
    "        value_t = P.critic(observation)\n",
    "        logprobability_t = P.logprobabilities(logits, action)\n",
    "\n",
    "        # Store obs, act, rew, v_t, logp_pi_t\n",
    "        P.buffer.store(observation, action, reward, value_t, logprobability_t)\n",
    "\n",
    "        # Update the observation\n",
    "        observation = np.asarray(observation_new)\n",
    "\n",
    "        # print(observation)\n",
    "        # print(type(observation))\n",
    "\n",
    "        # Finish trajectory if reached to a terminal state\n",
    "        terminal = done\n",
    "        if terminal or (t == steps_per_epoch - 1):\n",
    "            last_value = 0 if done else P.critic(observation.reshape(1, -1))\n",
    "            P.buffer.finish_trajectory(last_value)\n",
    "            sum_return += P.episode_return\n",
    "            sum_length += P.episode_length\n",
    "            num_episodes += 1\n",
    "            observation, episode_return, episode_length = env.reset(), 0, 0\n",
    "\n",
    "    # Get values from the buffer\n",
    "    (\n",
    "        observation_buffer,\n",
    "        action_buffer,\n",
    "        advantage_buffer,\n",
    "        return_buffer,\n",
    "        logprobability_buffer,\n",
    "    ) = P.buffer.get()\n",
    "\n",
    "    # Update the policy and implement early stopping using KL divergence\n",
    "    for _ in range(train_policy_iterations):\n",
    "        kl = P.train_policy(\n",
    "            observation_buffer, action_buffer, logprobability_buffer, advantage_buffer\n",
    "        )\n",
    "        if kl > 1.5 * target_kl:\n",
    "            # Early Stopping\n",
    "            break\n",
    "\n",
    "    # Update the value function\n",
    "    for _ in range(train_value_iterations):\n",
    "        P.train_value_function(observation_buffer, return_buffer)\n",
    "\n",
    "    timeEnd = time.time()  # End timer here\n",
    "    # Print average reward for each epoch\n",
    "    print(\n",
    "        f\"Epoch: {epoch}\"\n",
    "        f\"\\n\\tReward: {round(reward,2)}\"\n",
    "        f\"\\n\\tTime Elapsed: {round(timeEnd - timeStart)}s\\n\"\n",
    "    )\n",
    "\n",
    "    epochRewards.append(epochReward)\n",
    "    averageReward = sum(epochRewards[-movingRate:]) / len(epochRewards[-movingRate:])\n",
    "    movingEpRewards[\"average\"].append(averageReward)\n",
    "    movingEpRewards[\"epoch\"].append(epoch + 1)\n",
    "\n",
    "    # Save graph and model for every n number of epochs\n",
    "    if epoch % savePeriod == 0 and epoch != 0:\n",
    "        if saveResultsToPlot:\n",
    "            plt.plot(movingEpRewards['epoch'], movingEpRewards['average'], label=\"average reward\")\n",
    "            plt.title(\"Epoch \" + str(epoch) + \" Rewards\")\n",
    "            plt.xlabel(\"Epochs\")\n",
    "            plt.ylabel(\"Reward\")\n",
    "            plt.legend(loc=0)\n",
    "            plt.savefig(\"./Experiments/\" + str(experimentName) + \"/avgrewardplot\" + str(epoch) + \".png\")\n",
    "            plt.show()\n",
    "            plt.clf()\n",
    "\n",
    "        P.archive(epoch)\n",
    "        print(\"\\nSaved Model!\\n\")\n",
    "\n",
    "    timeStart = time.time()  # Start timer here\n",
    "\n",
    "print(\"<<<<<<<<<<<<<<<<<<<<DONE>>>>>>>>>>>>>>>>>>>>>\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
